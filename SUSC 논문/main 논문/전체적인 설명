멀티 포맷 캐시 인코딩을 활용한 LLM 모델 경량화 연구

1. 연구 배경
최근 대규모 언어 모델(LLM, Large Language Model)의 활용이 증가함에 따라 모델의 입력 모달리티가 확장되고 있다. 기존의 텍스트 기반 입력뿐만 아니라 이미지, 오디오 등의 데이터도 함께 처리하는 멀티모달 모델이 등장하고 있다.
이에 따라 모델의 연산량과 메모리 사용량이 급증하고 있는데 이러한 문제를 해결하기 위해 효율적인 메모리 관리 및 계산 최적화 기법이 필요하다.

2. 연구 목표
본 연구는 LLM의 경량화를 위해 멀티 포맷 캐시 인코딩(Multi-format Cache Encoding) 기법을 제안하고자 한다. 
이를 통해 KV(Key-Value) 캐시를 입력 데이터 타입(텍스트, 이미지, 오디오)에 따라 맞춤형으로 압축하고 최적화된 어텐션 메커니즘을 설계하여 연산 효율성을 개선하는 것이 목표이다. 
궁극적으로 입력 모달리티가 증가하더라도 모델이 일관된 성능을 유지하도록 한다.

3. 연구 내용
3.1. 입력 데이터의 타입별 특징 분석
LLM이 처리하는 입력 데이터는 크게 세 가지 타입으로 구분할 수 있다.
- 텍스트(Text): 단어 및 문장 단위로 토큰화된 데이터 일반적으로 압축 및 저장이 용이함.
- 이미지(Image): 픽셀 정보 및 특징 맵을 활용하여 표현되며 CNN 등의 방법과 결합 가능.
- 오디오(Audio): 주파수 변환 및 특성 추출을 통해 벡터화되며 시계열 데이터 특성을 가짐.

각 타입의 데이터는 저장 및 검색 시 서로 다른 특성을 가지므로 동일한 방식으로 KV 캐시를 구성하는 것은 비효율적이다.-> 타입별 최적화된 캐시 압축 방식을 도입할 필요가 있다.

3.2. KV 캐시의 타입별 맞춤 압축 기법
제안하는 멀티 포맷 캐시 인코딩 기법에서는 KV 캐시를 입력 데이터 타입에 따라 다르게 압축한다.
- 텍스트 데이터: 일반적인 양자화 및 행렬 축소 기법을 활용하여 압축.
- 이미지 데이터: 특성 맵의 공간 정보를 활용하여 압축률을 최적화.
- 오디오 데이터: 주파수 변환 기반의 압축 방식 적용 및 시계열 정보를 보존하는 압축 전략 사용.

이러한 접근 방식을 통해 KV 캐시의 메모리 사용량을 줄이고 연산량을 감소시켜 모델의 전반적인 속도를 개선할 수 있다.

3.3. 어텐션 메커니즘의 기여 및 최적화
본 연구에서는 멀티 포맷 캐시 인코딩을 적용함으로써 어텐션 메커니즘의 효율성을 개선하는 방법을 연구한다. -> KV 캐시의 압축이 어텐션 연산의 복잡도를 줄이는 데 기여할 것으로 예상된다.
- 메모리 사용량 감소: 캐시 크기를 줄임으로써 GPU/TPU 메모리 효율성을 극대화.
- 연산량 절감: 불필요한 연산을 줄여 어텐션 연산 속도 향상.
- 모달리티 증가 대응: 입력 데이터 타입이 증가하더라도 일관된 성능 유지.

4. 기대 효과
1. 메모리 절약: KV 캐시의 타입별 압축을 통해 불필요한 데이터 저장 공간을 절감.
2. 연산 효율성 개선: 적절한 데이터 구조를 활용하여 어텐션 메커니즘의 연산량을 줄임.
3. 확장성 향상: 다양한 입력 모달리티에 대해 유연하게 대응 가능.
4. 모델 경량화: 대규모 LLM을 경량화하여 모바일 및 엣지 디바이스에서도 활용 가능.(?)

5. 결론 및 향후 연구 방향
본 연구는 멀티 포맷 캐시 인코딩을 적용하여 LLM 모델의 경량화 및 효율적인 어텐션 메커니즘 설계를 목표로 한다. 이를 통해 입력 데이터의 다양성이 증가하더라도 일관된 성능을 유지할 수 있도록 하며 메모리 사용량 절감과 계산 효율성 향상을 기대할 수 있다. 
향후 연구에서는 실제 모델 적용을 통해 성능 평가를 진행하고 추가적인 압축 및 최적화 방법을 탐색할 계획이다.

